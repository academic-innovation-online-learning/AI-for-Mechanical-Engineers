{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8bfc2b9-969b-4b0e-bf41-e947a661ce1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "faf7a706-2bed-4e53-a8f4-0ef5d28e5f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55f5c308-72e0-4fd3-afa2-793cad0f84e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Generator with fewer layers and smaller output size (14x14)\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Define the Discriminator with smaller architecture\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc751609-27f5-44ea-8d1e-860ed3fdc951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "latent_dim = 100\n",
    "img_size = 14 * 14  # Reduced image size (14x14)\n",
    "batch_size = 64\n",
    "learning_rate = 0.0002\n",
    "num_epochs = 20  # Fewer epochs for faster training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1b3c043-7855-4219-995c-61928fc62526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the Data with image resizing to 14x14\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(14),  # Resize images to 14x14\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "921f4589-2829-41fb-b7ef-e7096a917198",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models, loss function, and optimizers\n",
    "generator = Generator(input_dim=latent_dim, output_dim=img_size)#.to('cuda')\n",
    "discriminator = Discriminator(input_dim=img_size)#.to('cuda')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer_G = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "optimizer_D = optim.Adam(discriminator.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "72ddea90-d0a5-4f80-9046-6337d963b8ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/20] Batch 0/938                   Loss D: 0.7247875332832336, loss G: 0.7251704335212708\n",
      "Epoch [0/20] Batch 100/938                   Loss D: 0.44265037775039673, loss G: 0.829223096370697\n",
      "Epoch [0/20] Batch 200/938                   Loss D: 0.1570224016904831, loss G: 1.646615743637085\n",
      "Epoch [0/20] Batch 300/938                   Loss D: 0.0791839137673378, loss G: 2.3788938522338867\n",
      "Epoch [0/20] Batch 400/938                   Loss D: 0.020174797624349594, loss G: 3.932560443878174\n",
      "Epoch [0/20] Batch 500/938                   Loss D: 0.10483415424823761, loss G: 2.281407117843628\n",
      "Epoch [0/20] Batch 600/938                   Loss D: 0.03514634072780609, loss G: 3.466336727142334\n",
      "Epoch [0/20] Batch 700/938                   Loss D: 0.4219626188278198, loss G: 1.58487868309021\n",
      "Epoch [0/20] Batch 800/938                   Loss D: 0.08626537024974823, loss G: 2.681753635406494\n",
      "Epoch [0/20] Batch 900/938                   Loss D: 0.060810744762420654, loss G: 3.6199655532836914\n",
      "Epoch [1/20] Batch 0/938                   Loss D: 0.03979052975773811, loss G: 3.0173895359039307\n",
      "Epoch [1/20] Batch 100/938                   Loss D: 0.04819469898939133, loss G: 3.3824381828308105\n",
      "Epoch [1/20] Batch 200/938                   Loss D: 0.09201498329639435, loss G: 2.3076632022857666\n",
      "Epoch [1/20] Batch 300/938                   Loss D: 0.4731625020503998, loss G: 1.2189868688583374\n",
      "Epoch [1/20] Batch 400/938                   Loss D: 0.40885549783706665, loss G: 1.2742669582366943\n",
      "Epoch [1/20] Batch 500/938                   Loss D: 0.36327576637268066, loss G: 1.7555361986160278\n",
      "Epoch [1/20] Batch 600/938                   Loss D: 0.1664222925901413, loss G: 2.5019423961639404\n",
      "Epoch [1/20] Batch 700/938                   Loss D: 0.16016069054603577, loss G: 2.3166425228118896\n",
      "Epoch [1/20] Batch 800/938                   Loss D: 0.16412535309791565, loss G: 2.232982873916626\n",
      "Epoch [1/20] Batch 900/938                   Loss D: 0.18203724920749664, loss G: 2.3541486263275146\n",
      "Epoch [2/20] Batch 0/938                   Loss D: 0.24595516920089722, loss G: 1.4353151321411133\n",
      "Epoch [2/20] Batch 100/938                   Loss D: 0.3692367374897003, loss G: 1.1942143440246582\n",
      "Epoch [2/20] Batch 200/938                   Loss D: 0.2819538116455078, loss G: 1.8823277950286865\n",
      "Epoch [2/20] Batch 300/938                   Loss D: 0.45012524724006653, loss G: 1.432133436203003\n",
      "Epoch [2/20] Batch 400/938                   Loss D: 0.3636312186717987, loss G: 1.4900693893432617\n",
      "Epoch [2/20] Batch 500/938                   Loss D: 0.18613044917583466, loss G: 2.0105700492858887\n",
      "Epoch [2/20] Batch 600/938                   Loss D: 0.6583716869354248, loss G: 1.136077880859375\n",
      "Epoch [2/20] Batch 700/938                   Loss D: 0.3692431151866913, loss G: 1.470908761024475\n",
      "Epoch [2/20] Batch 800/938                   Loss D: 0.5360922813415527, loss G: 1.1778444051742554\n",
      "Epoch [2/20] Batch 900/938                   Loss D: 0.19926635921001434, loss G: 2.374265432357788\n",
      "Epoch [3/20] Batch 0/938                   Loss D: 0.3360562324523926, loss G: 1.415757656097412\n",
      "Epoch [3/20] Batch 100/938                   Loss D: 0.3328385353088379, loss G: 1.938506007194519\n",
      "Epoch [3/20] Batch 200/938                   Loss D: 0.2680884599685669, loss G: 2.4428958892822266\n",
      "Epoch [3/20] Batch 300/938                   Loss D: 0.11883306503295898, loss G: 2.3997161388397217\n",
      "Epoch [3/20] Batch 400/938                   Loss D: 0.1594131886959076, loss G: 2.8755810260772705\n",
      "Epoch [3/20] Batch 500/938                   Loss D: 0.17121118307113647, loss G: 2.1158275604248047\n",
      "Epoch [3/20] Batch 600/938                   Loss D: 0.22429010272026062, loss G: 2.6163651943206787\n",
      "Epoch [3/20] Batch 700/938                   Loss D: 0.16650500893592834, loss G: 2.1023404598236084\n",
      "Epoch [3/20] Batch 800/938                   Loss D: 0.11866161227226257, loss G: 2.0561089515686035\n",
      "Epoch [3/20] Batch 900/938                   Loss D: 0.1685141623020172, loss G: 2.165971040725708\n",
      "Epoch [4/20] Batch 0/938                   Loss D: 0.18336251378059387, loss G: 1.737532377243042\n",
      "Epoch [4/20] Batch 100/938                   Loss D: 0.1469441056251526, loss G: 2.300624370574951\n",
      "Epoch [4/20] Batch 200/938                   Loss D: 0.14535996317863464, loss G: 2.591369152069092\n",
      "Epoch [4/20] Batch 300/938                   Loss D: 0.18698999285697937, loss G: 1.757308006286621\n",
      "Epoch [4/20] Batch 400/938                   Loss D: 0.21525532007217407, loss G: 2.8271119594573975\n",
      "Epoch [4/20] Batch 500/938                   Loss D: 0.22841660678386688, loss G: 2.6479804515838623\n",
      "Epoch [4/20] Batch 600/938                   Loss D: 0.25234121084213257, loss G: 2.155165672302246\n",
      "Epoch [4/20] Batch 700/938                   Loss D: 0.10536855459213257, loss G: 2.675295352935791\n",
      "Epoch [4/20] Batch 800/938                   Loss D: 0.115362748503685, loss G: 3.4407591819763184\n",
      "Epoch [4/20] Batch 900/938                   Loss D: 0.32962703704833984, loss G: 2.932969570159912\n",
      "Epoch [5/20] Batch 0/938                   Loss D: 0.19244599342346191, loss G: 2.6743533611297607\n",
      "Epoch [5/20] Batch 100/938                   Loss D: 0.11334294080734253, loss G: 2.6041765213012695\n",
      "Epoch [5/20] Batch 200/938                   Loss D: 0.1884075254201889, loss G: 3.006838083267212\n",
      "Epoch [5/20] Batch 300/938                   Loss D: 0.15764595568180084, loss G: 3.0628671646118164\n",
      "Epoch [5/20] Batch 400/938                   Loss D: 0.11801692098379135, loss G: 2.5761754512786865\n",
      "Epoch [5/20] Batch 500/938                   Loss D: 0.37481963634490967, loss G: 2.0732977390289307\n",
      "Epoch [5/20] Batch 600/938                   Loss D: 0.14903011918067932, loss G: 2.8828046321868896\n",
      "Epoch [5/20] Batch 700/938                   Loss D: 0.15022200345993042, loss G: 2.2173852920532227\n",
      "Epoch [5/20] Batch 800/938                   Loss D: 0.2016279399394989, loss G: 2.360640287399292\n",
      "Epoch [5/20] Batch 900/938                   Loss D: 0.20107805728912354, loss G: 2.0327844619750977\n",
      "Epoch [6/20] Batch 0/938                   Loss D: 0.36131367087364197, loss G: 2.225358009338379\n",
      "Epoch [6/20] Batch 100/938                   Loss D: 0.13322854042053223, loss G: 3.050692558288574\n",
      "Epoch [6/20] Batch 200/938                   Loss D: 0.2507799565792084, loss G: 1.9862815141677856\n",
      "Epoch [6/20] Batch 300/938                   Loss D: 0.21050044894218445, loss G: 2.8999171257019043\n",
      "Epoch [6/20] Batch 400/938                   Loss D: 0.32378655672073364, loss G: 1.825890302658081\n",
      "Epoch [6/20] Batch 500/938                   Loss D: 0.1425919532775879, loss G: 2.1525092124938965\n",
      "Epoch [6/20] Batch 600/938                   Loss D: 0.2364128977060318, loss G: 2.6097412109375\n",
      "Epoch [6/20] Batch 700/938                   Loss D: 0.12954111397266388, loss G: 3.5641026496887207\n",
      "Epoch [6/20] Batch 800/938                   Loss D: 0.390488862991333, loss G: 1.6944876909255981\n",
      "Epoch [6/20] Batch 900/938                   Loss D: 0.1546659767627716, loss G: 3.93613338470459\n",
      "Epoch [7/20] Batch 0/938                   Loss D: 0.19053411483764648, loss G: 3.285835027694702\n",
      "Epoch [7/20] Batch 100/938                   Loss D: 0.10202586650848389, loss G: 2.4972949028015137\n",
      "Epoch [7/20] Batch 200/938                   Loss D: 0.21874180436134338, loss G: 3.1118929386138916\n",
      "Epoch [7/20] Batch 300/938                   Loss D: 0.20508898794651031, loss G: 2.637357711791992\n",
      "Epoch [7/20] Batch 400/938                   Loss D: 0.17812460660934448, loss G: 2.6415324211120605\n",
      "Epoch [7/20] Batch 500/938                   Loss D: 0.2614842355251312, loss G: 2.1724088191986084\n",
      "Epoch [7/20] Batch 600/938                   Loss D: 0.1616029292345047, loss G: 3.0176382064819336\n",
      "Epoch [7/20] Batch 700/938                   Loss D: 0.13917085528373718, loss G: 2.9994451999664307\n",
      "Epoch [7/20] Batch 800/938                   Loss D: 0.38415855169296265, loss G: 2.6883769035339355\n",
      "Epoch [7/20] Batch 900/938                   Loss D: 0.10276444256305695, loss G: 2.833207607269287\n",
      "Epoch [8/20] Batch 0/938                   Loss D: 0.31904682517051697, loss G: 2.800164222717285\n",
      "Epoch [8/20] Batch 100/938                   Loss D: 0.16134831309318542, loss G: 2.714517593383789\n",
      "Epoch [8/20] Batch 200/938                   Loss D: 0.13963207602500916, loss G: 3.504646062850952\n",
      "Epoch [8/20] Batch 300/938                   Loss D: 0.13486842811107635, loss G: 2.5818824768066406\n",
      "Epoch [8/20] Batch 400/938                   Loss D: 0.11312374472618103, loss G: 3.1393423080444336\n",
      "Epoch [8/20] Batch 500/938                   Loss D: 0.15463396906852722, loss G: 2.6586835384368896\n",
      "Epoch [8/20] Batch 600/938                   Loss D: 0.2219536155462265, loss G: 2.7099924087524414\n",
      "Epoch [8/20] Batch 700/938                   Loss D: 0.2263503074645996, loss G: 2.9087440967559814\n",
      "Epoch [8/20] Batch 800/938                   Loss D: 0.12741714715957642, loss G: 3.1869823932647705\n",
      "Epoch [8/20] Batch 900/938                   Loss D: 0.09232993423938751, loss G: 3.233769655227661\n",
      "Epoch [9/20] Batch 0/938                   Loss D: 0.19654741883277893, loss G: 3.0169999599456787\n",
      "Epoch [9/20] Batch 100/938                   Loss D: 0.12473513185977936, loss G: 3.8542680740356445\n",
      "Epoch [9/20] Batch 200/938                   Loss D: 0.10899950563907623, loss G: 3.621298313140869\n",
      "Epoch [9/20] Batch 300/938                   Loss D: 0.09490553289651871, loss G: 2.955336570739746\n",
      "Epoch [9/20] Batch 400/938                   Loss D: 0.14392578601837158, loss G: 3.613642930984497\n",
      "Epoch [9/20] Batch 500/938                   Loss D: 0.10629284381866455, loss G: 2.8976097106933594\n",
      "Epoch [9/20] Batch 600/938                   Loss D: 0.0828038901090622, loss G: 3.813565731048584\n",
      "Epoch [9/20] Batch 700/938                   Loss D: 0.141228586435318, loss G: 2.9212565422058105\n",
      "Epoch [9/20] Batch 800/938                   Loss D: 0.3753069043159485, loss G: 1.1689298152923584\n",
      "Epoch [9/20] Batch 900/938                   Loss D: 0.062410615384578705, loss G: 3.576862096786499\n",
      "Epoch [10/20] Batch 0/938                   Loss D: 0.05037686228752136, loss G: 3.968592643737793\n",
      "Epoch [10/20] Batch 100/938                   Loss D: 0.13756562769412994, loss G: 4.04194974899292\n",
      "Epoch [10/20] Batch 200/938                   Loss D: 0.12195083498954773, loss G: 3.67366361618042\n",
      "Epoch [10/20] Batch 300/938                   Loss D: 0.1802803874015808, loss G: 3.5843117237091064\n",
      "Epoch [10/20] Batch 400/938                   Loss D: 0.21038129925727844, loss G: 4.308743476867676\n",
      "Epoch [10/20] Batch 500/938                   Loss D: 0.11929519474506378, loss G: 3.585721731185913\n",
      "Epoch [10/20] Batch 600/938                   Loss D: 0.24419446289539337, loss G: 2.415436029434204\n",
      "Epoch [10/20] Batch 700/938                   Loss D: 0.2862984836101532, loss G: 2.34768009185791\n",
      "Epoch [10/20] Batch 800/938                   Loss D: 0.11427272856235504, loss G: 3.154390573501587\n",
      "Epoch [10/20] Batch 900/938                   Loss D: 0.09583023935556412, loss G: 2.992980480194092\n",
      "Epoch [11/20] Batch 0/938                   Loss D: 0.21341323852539062, loss G: 3.3623294830322266\n",
      "Epoch [11/20] Batch 100/938                   Loss D: 0.14605611562728882, loss G: 3.0732364654541016\n",
      "Epoch [11/20] Batch 200/938                   Loss D: 0.159437358379364, loss G: 2.5099542140960693\n",
      "Epoch [11/20] Batch 300/938                   Loss D: 0.11764465272426605, loss G: 3.4403154850006104\n",
      "Epoch [11/20] Batch 400/938                   Loss D: 0.24291512370109558, loss G: 2.253307580947876\n",
      "Epoch [11/20] Batch 500/938                   Loss D: 0.10038534551858902, loss G: 3.1520254611968994\n",
      "Epoch [11/20] Batch 600/938                   Loss D: 0.207670658826828, loss G: 3.5687925815582275\n",
      "Epoch [11/20] Batch 700/938                   Loss D: 0.32179614901542664, loss G: 3.266356945037842\n",
      "Epoch [11/20] Batch 800/938                   Loss D: 0.16956359148025513, loss G: 2.5722575187683105\n",
      "Epoch [11/20] Batch 900/938                   Loss D: 0.13582949340343475, loss G: 3.528992176055908\n",
      "Epoch [12/20] Batch 0/938                   Loss D: 0.20782622694969177, loss G: 1.9470393657684326\n",
      "Epoch [12/20] Batch 100/938                   Loss D: 0.13766777515411377, loss G: 2.206904888153076\n",
      "Epoch [12/20] Batch 200/938                   Loss D: 0.2453714907169342, loss G: 2.6000070571899414\n",
      "Epoch [12/20] Batch 300/938                   Loss D: 0.23802049458026886, loss G: 2.7790064811706543\n",
      "Epoch [12/20] Batch 400/938                   Loss D: 0.2590022385120392, loss G: 2.8443210124969482\n",
      "Epoch [12/20] Batch 500/938                   Loss D: 0.23535461723804474, loss G: 2.5163543224334717\n",
      "Epoch [12/20] Batch 600/938                   Loss D: 0.2595600485801697, loss G: 2.082751750946045\n",
      "Epoch [12/20] Batch 700/938                   Loss D: 0.18374450504779816, loss G: 2.6327550411224365\n",
      "Epoch [12/20] Batch 800/938                   Loss D: 0.24101340770721436, loss G: 2.085552215576172\n",
      "Epoch [12/20] Batch 900/938                   Loss D: 0.2220010757446289, loss G: 2.6549196243286133\n",
      "Epoch [13/20] Batch 0/938                   Loss D: 0.30183759331703186, loss G: 2.4483048915863037\n",
      "Epoch [13/20] Batch 100/938                   Loss D: 0.32127711176872253, loss G: 3.497283935546875\n",
      "Epoch [13/20] Batch 200/938                   Loss D: 0.2001999020576477, loss G: 2.598726272583008\n",
      "Epoch [13/20] Batch 300/938                   Loss D: 0.18731343746185303, loss G: 2.147348403930664\n",
      "Epoch [13/20] Batch 400/938                   Loss D: 0.26362133026123047, loss G: 2.645477771759033\n",
      "Epoch [13/20] Batch 500/938                   Loss D: 0.41949889063835144, loss G: 2.823676347732544\n",
      "Epoch [13/20] Batch 600/938                   Loss D: 0.2823523283004761, loss G: 2.076154947280884\n",
      "Epoch [13/20] Batch 700/938                   Loss D: 0.2459610402584076, loss G: 2.3045248985290527\n",
      "Epoch [13/20] Batch 800/938                   Loss D: 0.2937003970146179, loss G: 2.524919271469116\n",
      "Epoch [13/20] Batch 900/938                   Loss D: 0.18623268604278564, loss G: 2.5402872562408447\n",
      "Epoch [14/20] Batch 0/938                   Loss D: 0.191201314330101, loss G: 1.9008846282958984\n",
      "Epoch [14/20] Batch 100/938                   Loss D: 0.22443397343158722, loss G: 2.3373517990112305\n",
      "Epoch [14/20] Batch 200/938                   Loss D: 0.2093326896429062, loss G: 2.4544286727905273\n",
      "Epoch [14/20] Batch 300/938                   Loss D: 0.13050368428230286, loss G: 2.704854965209961\n",
      "Epoch [14/20] Batch 400/938                   Loss D: 0.19666409492492676, loss G: 2.2868902683258057\n",
      "Epoch [14/20] Batch 500/938                   Loss D: 0.22901830077171326, loss G: 3.0810155868530273\n",
      "Epoch [14/20] Batch 600/938                   Loss D: 0.21045538783073425, loss G: 2.7530946731567383\n",
      "Epoch [14/20] Batch 700/938                   Loss D: 0.3664526343345642, loss G: 2.2669544219970703\n",
      "Epoch [14/20] Batch 800/938                   Loss D: 0.24812057614326477, loss G: 2.4015860557556152\n",
      "Epoch [14/20] Batch 900/938                   Loss D: 0.2731800973415375, loss G: 2.320526599884033\n",
      "Epoch [15/20] Batch 0/938                   Loss D: 0.19862616062164307, loss G: 2.8425862789154053\n",
      "Epoch [15/20] Batch 100/938                   Loss D: 0.23469004034996033, loss G: 2.272923231124878\n",
      "Epoch [15/20] Batch 200/938                   Loss D: 0.2990846037864685, loss G: 2.3506577014923096\n",
      "Epoch [15/20] Batch 300/938                   Loss D: 0.4159296452999115, loss G: 2.1621031761169434\n",
      "Epoch [15/20] Batch 400/938                   Loss D: 0.21856585144996643, loss G: 2.2977895736694336\n",
      "Epoch [15/20] Batch 500/938                   Loss D: 0.26912814378738403, loss G: 2.7307751178741455\n",
      "Epoch [15/20] Batch 600/938                   Loss D: 0.21097375452518463, loss G: 2.708122968673706\n",
      "Epoch [15/20] Batch 700/938                   Loss D: 0.3622558116912842, loss G: 1.6721816062927246\n",
      "Epoch [15/20] Batch 800/938                   Loss D: 0.20262104272842407, loss G: 2.3674914836883545\n",
      "Epoch [15/20] Batch 900/938                   Loss D: 0.31297433376312256, loss G: 2.5331149101257324\n",
      "Epoch [16/20] Batch 0/938                   Loss D: 0.2615184485912323, loss G: 2.787426233291626\n",
      "Epoch [16/20] Batch 100/938                   Loss D: 0.2522280216217041, loss G: 2.0892393589019775\n",
      "Epoch [16/20] Batch 200/938                   Loss D: 0.33903202414512634, loss G: 2.725153923034668\n",
      "Epoch [16/20] Batch 300/938                   Loss D: 0.31061720848083496, loss G: 1.943711519241333\n",
      "Epoch [16/20] Batch 400/938                   Loss D: 0.503921627998352, loss G: 2.424698829650879\n",
      "Epoch [16/20] Batch 500/938                   Loss D: 0.4003481864929199, loss G: 1.9498960971832275\n",
      "Epoch [16/20] Batch 600/938                   Loss D: 0.2826560139656067, loss G: 1.6373465061187744\n",
      "Epoch [16/20] Batch 700/938                   Loss D: 0.2889384925365448, loss G: 2.3998212814331055\n",
      "Epoch [16/20] Batch 800/938                   Loss D: 0.29287758469581604, loss G: 1.7125200033187866\n",
      "Epoch [16/20] Batch 900/938                   Loss D: 0.4129418134689331, loss G: 1.8415707349777222\n",
      "Epoch [17/20] Batch 0/938                   Loss D: 0.28515884280204773, loss G: 2.0640666484832764\n",
      "Epoch [17/20] Batch 100/938                   Loss D: 0.36276519298553467, loss G: 2.2360875606536865\n",
      "Epoch [17/20] Batch 200/938                   Loss D: 0.3774336874485016, loss G: 1.7356233596801758\n",
      "Epoch [17/20] Batch 300/938                   Loss D: 0.3955627381801605, loss G: 2.5468220710754395\n",
      "Epoch [17/20] Batch 400/938                   Loss D: 0.24995183944702148, loss G: 2.0754880905151367\n",
      "Epoch [17/20] Batch 500/938                   Loss D: 0.42756855487823486, loss G: 2.0095579624176025\n",
      "Epoch [17/20] Batch 600/938                   Loss D: 0.3421245813369751, loss G: 1.750032901763916\n",
      "Epoch [17/20] Batch 700/938                   Loss D: 0.17737779021263123, loss G: 2.356976270675659\n",
      "Epoch [17/20] Batch 800/938                   Loss D: 0.3187665343284607, loss G: 1.402695655822754\n",
      "Epoch [17/20] Batch 900/938                   Loss D: 0.3966018557548523, loss G: 1.363217830657959\n",
      "Epoch [18/20] Batch 0/938                   Loss D: 0.23097163438796997, loss G: 1.9654192924499512\n",
      "Epoch [18/20] Batch 100/938                   Loss D: 0.4323479235172272, loss G: 2.1627790927886963\n",
      "Epoch [18/20] Batch 200/938                   Loss D: 0.3896993100643158, loss G: 2.9922049045562744\n",
      "Epoch [18/20] Batch 300/938                   Loss D: 0.276488721370697, loss G: 2.0394515991210938\n",
      "Epoch [18/20] Batch 400/938                   Loss D: 0.3162098228931427, loss G: 2.082540273666382\n",
      "Epoch [18/20] Batch 500/938                   Loss D: 0.3785659670829773, loss G: 2.2401809692382812\n",
      "Epoch [18/20] Batch 600/938                   Loss D: 0.4433988928794861, loss G: 1.5651837587356567\n",
      "Epoch [18/20] Batch 700/938                   Loss D: 0.3765990138053894, loss G: 2.050802230834961\n",
      "Epoch [18/20] Batch 800/938                   Loss D: 0.294134259223938, loss G: 2.567564010620117\n",
      "Epoch [18/20] Batch 900/938                   Loss D: 0.48738425970077515, loss G: 1.6906627416610718\n",
      "Epoch [19/20] Batch 0/938                   Loss D: 0.40493836998939514, loss G: 2.9808294773101807\n",
      "Epoch [19/20] Batch 100/938                   Loss D: 0.2840462923049927, loss G: 2.1551663875579834\n",
      "Epoch [19/20] Batch 200/938                   Loss D: 0.269056499004364, loss G: 2.1144754886627197\n",
      "Epoch [19/20] Batch 300/938                   Loss D: 0.47433438897132874, loss G: 1.8177485466003418\n",
      "Epoch [19/20] Batch 400/938                   Loss D: 0.3823142349720001, loss G: 2.141376495361328\n",
      "Epoch [19/20] Batch 500/938                   Loss D: 0.25489211082458496, loss G: 2.368333578109741\n",
      "Epoch [19/20] Batch 600/938                   Loss D: 0.2991141080856323, loss G: 1.4791624546051025\n",
      "Epoch [19/20] Batch 700/938                   Loss D: 0.24033787846565247, loss G: 2.3376638889312744\n",
      "Epoch [19/20] Batch 800/938                   Loss D: 0.4220728874206543, loss G: 2.5482873916625977\n",
      "Epoch [19/20] Batch 900/938                   Loss D: 0.3560304343700409, loss G: 2.4771370887756348\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (imgs, _) in enumerate(train_loader):\n",
    "        \n",
    "        # Adversarial ground truths\n",
    "        valid = torch.ones((imgs.size(0), 1), requires_grad=False)#.to('cuda')\n",
    "        fake = torch.zeros((imgs.size(0), 1), requires_grad=False)#.to('cuda')\n",
    "        \n",
    "        # Configure input\n",
    "        real_imgs = imgs.view(imgs.size(0), -1)#.to('cuda')\n",
    "        \n",
    "        # Train Generator\n",
    "        optimizer_G.zero_grad()\n",
    "        z = torch.randn((imgs.size(0), latent_dim))#.to('cuda')\n",
    "        gen_imgs = generator(z)\n",
    "        g_loss = criterion(discriminator(gen_imgs), valid)\n",
    "        g_loss.backward()\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        # Train Discriminator\n",
    "        optimizer_D.zero_grad()\n",
    "        real_loss = criterion(discriminator(real_imgs), valid)\n",
    "        fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
    "        d_loss = (real_loss + fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # Print progress\n",
    "        if i % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] Batch {i}/{len(train_loader)} \\\n",
    "                  Loss D: {d_loss.item()}, loss G: {g_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f8aace-b34a-4dfb-b6f2-62c946cb0809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing Generated and Original Images\n",
    "def show_images(gen_images, real_images):\n",
    "    gen_images = gen_images.view(gen_images.size(0), 1, 14, 14).cpu().data\n",
    "    real_images = real_images.view(real_images.size(0), 1, 14, 14).cpu().data\n",
    "    \n",
    "    # Concatenate generated and real images\n",
    "    images = torch.cat([gen_images, real_images])\n",
    "    \n",
    "    grid = torchvision.utils.make_grid(images, nrow=8, normalize=True)\n",
    "    \n",
    "    # Set smaller figure size\n",
    "    plt.figure(figsize=(6, 6))  # Smaller figure\n",
    "    plt.imshow(grid.permute(1, 2, 0))\n",
    "    plt.title('Top Half: Generated Images, Bottom Half: Original Images')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# Generate images\n",
    "z = torch.randn(16, latent_dim)#.to('cuda')  # Reduced to 16 for a smaller plot\n",
    "gen_imgs = generator(z)\n",
    "\n",
    "# Get a batch of real images\n",
    "real_imgs, _ = next(iter(train_loader))\n",
    "real_imgs = real_imgs[:16].view(16, -1)#.to('cuda')  # Reduced to 16 for a smaller plot\n",
    "\n",
    "# Show generated vs original images\n",
    "show_images(gen_imgs, real_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
