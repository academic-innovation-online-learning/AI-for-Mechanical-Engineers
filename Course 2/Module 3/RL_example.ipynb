{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda20d8b-6789-45a0-9afb-d62b241371e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "# Define the environment\n",
    "grid_size = 5\n",
    "goal_state = (4, 4)\n",
    "obstacle_state = (2, 2)\n",
    "start_state = (0, 0)\n",
    "\n",
    "# Rewards: +1 for goal, -1 for obstacle, 0 for everything else\n",
    "rewards = np.zeros((grid_size, grid_size))\n",
    "rewards[goal_state] = 1\n",
    "rewards[obstacle_state] = -1\n",
    "\n",
    "# Actions: up, down, left, right\n",
    "actions = ['up', 'down', 'left', 'right']\n",
    "action_mapping = {\n",
    "    'up': (-1, 0),\n",
    "    'down': (1, 0),\n",
    "    'left': (0, -1),\n",
    "    'right': (0, 1)\n",
    "}\n",
    "action_arrows = {\n",
    "    'up': (0, 0.3),\n",
    "    'down': (0, -0.3),\n",
    "    'left': (-0.3, 0),\n",
    "    'right': (0.3, 0)\n",
    "}\n",
    "\n",
    "# Initialize Q-values table\n",
    "Q_values = np.zeros((grid_size, grid_size, len(actions)))\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.9  # Discount factor\n",
    "epsilon = 0.1  # Exploration rate\n",
    "episodes = 500  # Number of episodes\n",
    "\n",
    "# Helper function: get next state based on action\n",
    "def get_next_state(state, action):\n",
    "    delta = action_mapping[action]\n",
    "    next_state = (state[0] + delta[0], state[1] + delta[1])\n",
    "    \n",
    "    # Check boundaries (stay within grid)\n",
    "    next_state = (max(0, min(grid_size - 1, next_state[0])),\n",
    "                  max(0, min(grid_size - 1, next_state[1])))\n",
    "    \n",
    "    return next_state\n",
    "\n",
    "# Helper function: choose action using epsilon-greedy strategy\n",
    "def choose_action(state):\n",
    "    if random.uniform(0, 1) < epsilon:\n",
    "        # Exploration: choose random action\n",
    "        return random.choice(actions)\n",
    "    else:\n",
    "        # Exploitation: choose action with max Q-value\n",
    "        state_Q_values = Q_values[state[0], state[1], :]\n",
    "        return actions[np.argmax(state_Q_values)]\n",
    "\n",
    "# Q-learning algorithm\n",
    "for episode in range(episodes):\n",
    "    state = start_state\n",
    "    \n",
    "    while state != goal_state:\n",
    "        action = choose_action(state)\n",
    "        next_state = get_next_state(state, action)\n",
    "        reward = rewards[next_state]\n",
    "        \n",
    "        # Q-learning update\n",
    "        current_q_value = Q_values[state[0], state[1], actions.index(action)]\n",
    "        max_future_q_value = np.max(Q_values[next_state[0], next_state[1], :])\n",
    "        \n",
    "        new_q_value = current_q_value + alpha * (reward + gamma * max_future_q_value - current_q_value)\n",
    "        Q_values[state[0], state[1], actions.index(action)] = new_q_value\n",
    "        \n",
    "        # Move to next state\n",
    "        state = next_state\n",
    "\n",
    "# Visualization of learned Q-values\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Sum Q-values for each state to visualize overall state values\n",
    "state_values = np.max(Q_values, axis=2)\n",
    "\n",
    "# Plot heatmap of state values\n",
    "sns.heatmap(state_values, annot=True, cmap='coolwarm', cbar=True, fmt=\".2f\", ax=ax)\n",
    "\n",
    "# Highlight the goal and obstacle\n",
    "ax.add_patch(plt.Rectangle(goal_state, 1, 1, fill=False, edgecolor='green', lw=3))\n",
    "ax.add_patch(plt.Rectangle(obstacle_state, 1, 1, fill=False, edgecolor='red', lw=3))\n",
    "\n",
    "# Adding arrows for the learned policy\n",
    "for i in range(grid_size):\n",
    "    for j in range(grid_size):\n",
    "        if (i, j) != goal_state and (i, j) != obstacle_state:\n",
    "            best_action_idx = np.argmax(Q_values[i, j, :])\n",
    "            best_action = actions[best_action_idx]\n",
    "            arrow = action_arrows[best_action]\n",
    "            ax.arrow(j + 0.5, i + 0.5, arrow[0], -arrow[1], head_width=0.2, head_length=0.2, fc='black', ec='black')\n",
    "\n",
    "plt.title('Q-values Heatmap with Optimal Policy Arrows')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62682163-9e77-4dbe-937f-fa3d3b514f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
